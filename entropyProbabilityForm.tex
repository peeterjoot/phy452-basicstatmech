%
% Copyright © 2013 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
%\input{../blogpost.tex}
%\renewcommand{\basename}{entropyProbabilityForm}
%\renewcommand{\dirname}{notes/phy452/}
%\newcommand{\keywords}{Statistical mechanics, PHY452H1S, entropy, partition function, probability, grand partition function, average energy, average number of particles, free energy}
%
%\input{../peeter_prologue_print2.tex}
%
%\beginArtNoToc
%
%\generatetitle{probability forms of entropy}
%\chapter{probability forms of entropy}
\label{chap:entropyProbabilityForm}

\makeproblem{Entropy as probability}{pr:entropyProbabilityForm:1}{

\citep{jackson2000equilibrium} points out that entropy can be written as
\begin{dmath}\label{eqn:entropyProbabilityForm:20}
S = - \kB \sum_i P_i \ln P_i,
\end{dmath}
where
\begin{subequations}
\begin{dmath}\label{eqn:entropyProbabilityForm:40}
P_i = \frac{e^{-\beta E_i}}{Z}
\end{dmath}
\begin{dmath}\label{eqn:entropyProbabilityForm:60}
Z = \sum_i e^{-\beta E_i}.
\end{dmath}
\end{subequations}
Show that this follows from the free energy \(F = U - T S = -\kB \ln Z\).
} % makeproblem

\makeanswer{pr:entropyProbabilityForm:1}{

In terms of the free and average energies, we have
\begin{dmath}\label{eqn:entropyProbabilityForm:80}
\frac{S}{\kB}
= \frac{U - F}{\kB T}
=   \beta \lr{ -\PD{\beta}{\ln Z}}
  - \beta \lr{-\kB T \ln Z}
= \frac{\sum_i \beta E_i e^{-\beta E_i}}{Z}
  +\ln Z
= -\sum_i P_i \ln e^{-\beta E_i} + \sum_i P_i \ln Z
= -\sum_i P_i \ln \frac{e^{-\beta E_i}}{Z} P_i
= -\sum_i P_i \ln P_i.
\end{dmath}

} % makeanswer

\makeoproblem{Entropy in terms of grand partition probabilities}{pr:entropyProbabilityForm:2}{
\citep{pathriastatistical} pr 4.1,
\citep{jackson2000equilibrium} pr 3.15
}{

Generalize \cref{pr:entropyProbabilityForm:1} to the grand canonical scheme, where we have
\begin{subequations}
\begin{equation}\label{eqn:entropyProbabilityForm:100}
P_{r, s} = \frac{e^{-\alpha N_r - \beta E_s}}{\ZG}
\end{equation}
\begin{equation}\label{eqn:entropyProbabilityForm:120}
\ZG = \sum_{r,s} e^{-\alpha N_r - \beta E_s}
\end{equation}
\begin{equation}\label{eqn:entropyProbabilityForm:140}
z = e^{-\alpha} = e^{\mu \beta}
\end{equation}
\begin{equation}\label{eqn:entropyProbabilityForm:160}
q = \ln \ZG,
\end{equation}
\end{subequations}
and show
\begin{dmath}\label{eqn:entropyProbabilityForm:180}
S = - \kB \sum_{r,s} P_{r,s} \ln P_{r,s}.
\end{dmath}
} % makeoproblem

\makeanswer{pr:entropyProbabilityForm:2}{

With
\begin{equation}\label{eqn:entropyProbabilityForm:200}
\beta P V = q,
\end{equation}
the free energy takes the form
\begin{equation}\label{eqn:entropyProbabilityForm:220}
F = N \mu - P V = N \mu - q/\beta,
\end{equation}
so that the entropy (scaled by \(\kB\)) leads us to the desired result
\begin{dmath}\label{eqn:entropyProbabilityForm:240}
\frac{S}{\kB}
= \beta U - N \mu \beta + q/(\beta \kB T)
= -\beta \PD{\beta}{q} - z \mu \beta \PD{z}{q} + q
= \inv{\ZG}
\sum_{r, s}
\lr{
-\beta (-E_s) - \mu \beta N_r
} e^{-\alpha N_r - \beta E_s}
+
\ln \ZG
=
\sum_{r, s}
\ln e^{ \alpha N_r + \beta E_s } P_{r,s}
+
\lr{ \sum_{r, s} P_{r, s} }
\ln \ZG
=
-\sum_{r, s}
\ln
\frac{e^{ -\alpha N_r - \beta E_s }}{\ZG} P_{r,s}
=
-\sum_{r, s}
P_{r, s} \ln P_{r, s}
\end{dmath}

} % makeanswer

%\EndArticle
